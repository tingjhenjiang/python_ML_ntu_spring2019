{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用RNN做情意分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb # 讀入 IMDB 電影數據庫\n",
    "#fix loading imdb dataset problem: https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_words = 10000 #自然語言處理領域的問題中決定要用到多少字\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=total_num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 送入神經網路的輸入處理\n",
    "\n",
    "老師建議：雖然 RNN 是可以處理不同長度的輸入, 在寫程式時還是要\n",
    "\n",
    "* 設輸入文字長度的上限\n",
    "* 把每段文字都弄成一樣長, 太短的後面補上 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找出長度最長的影評\n",
    "vectorized_len = np.vectorize(len)\n",
    "length_of_each_comment = vectorized_len(x_train)\n",
    "pad_until_length = np.amax(length_of_each_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=pad_until_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=pad_until_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定神經網路架構\n",
    "\n",
    "* 將 10000 維的文字壓到 O 維 by \"word embedding\" ；避免用1-hot 處理 1 萬個字造成用 1 萬維的向量表示：浪費記憶空間\n",
    "* 然後用 O 個 LSTM\n",
    "* 最後一個 output, 直接用 sigmoid 送出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\SecSoftware\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               167400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 1,447,551\n",
      "Trainable params: 1,447,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_num_words, 128)) #目標壓縮成多少維度\n",
    "model.add(LSTM(150)) #幾個神經元\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\SecSoftware\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 299s 12ms/step - loss: 0.4230 - acc: 0.8020\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 328s 13ms/step - loss: 0.2651 - acc: 0.8928\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 423s 17ms/step - loss: 0.1954 - acc: 0.9259\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 386s 15ms/step - loss: 0.1427 - acc: 0.9475\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 291s 12ms/step - loss: 0.0991 - acc: 0.9654\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 335s 13ms/step - loss: 0.0755 - acc: 0.9748\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 332s 13ms/step - loss: 0.0520 - acc: 0.9826\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 334s 13ms/step - loss: 0.0465 - acc: 0.9845\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 396s 16ms/step - loss: 0.0327 - acc: 0.9900\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 373s 15ms/step - loss: 0.0348 - acc: 0.9890\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 438s 18ms/step - loss: 0.0277 - acc: 0.9916\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 474s 19ms/step - loss: 0.0167 - acc: 0.9949\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 452s 18ms/step - loss: 0.0184 - acc: 0.9944\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 356s 14ms/step - loss: 0.0161 - acc: 0.9949\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 265s 11ms/step - loss: 0.0156 - acc: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x208cefd3048>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "         batch_size=32,\n",
    "         epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢視結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測情形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1357s 54ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料的 loss 0.8484413851189614\n",
      "測試資料的正確率 0.85104\n"
     ]
    }
   ],
   "source": [
    "print('測試資料的 loss', score[0])\n",
    "print('測試資料的正確率', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始評論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id={k:(v+INDEX_FROM-1) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "def translate_and_show_comment(comment_i, comments, dictionary):\n",
    "    print(' '.join(dictionary[id] for id in comments[comment_i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 儲存結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfilename = 'wk10_RNN_model_for_imdb.h5'\n",
    "#model.save(modelfilename)\n",
    "from keras.models import load_model\n",
    "model = load_model(modelfilename)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
