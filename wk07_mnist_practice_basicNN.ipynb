{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "wk07_mnist_practice_basicNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tingjhenjiang/python_ML_ntu_spring2019/blob/master/wk07_mnist_practice_basicNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkEOCPm-INVk"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN2jgYrIIL5Z"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bXa5qpnIL5m"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEs7koA9IL5y"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_train = (x_train-x_train.min())/(x_train.max()-x_train.min())\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_test = (x_test-x_test.min())/(x_test.max() - x_test.min())\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=784))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='mse', optimizer=SGD(lr=0.087), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaL44Ly-IL54"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3HADXdAIL6C"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI3Ggu6cIL6I"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=100, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlYymaKwISaR"
      },
      "source": [
        "# Keras in Tensorflow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deggmNsKIVBh",
        "outputId": "6c29ef29-6029-4790-9435-760410b432eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tensorflow.keras import datasets #mnist\n",
        "#from tensorflow.keras.utils import np_utils\n",
        "from tensorflow.keras import models #Sequential\n",
        "from tensorflow.keras import layers #Dense, Activation\n",
        "from tensorflow.keras import optimizers #SGD\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7iqVDinIY9c",
        "outputId": "7bbc52df-3c7a-4c57-8dca-914d0b3c9520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "def std_nparray(arr):\n",
        "    arr = (arr-arr.min())/(arr.max()-arr.min())\n",
        "    return(arr)\n",
        "\n",
        "x_train = x_train.reshape(-1,784) #x_train.reshape(60000, 784)\n",
        "x_train = std_nparray(x_train)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "x_test = std_nparray(x_test)\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_train.dtype)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 784)\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRzxjtdwEM5k",
        "outputId": "8cfb9e89-4e84-4532-f429-18c10d780e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Dense(64,activation='relu',input_shape=(784,)))\n",
        "model.add(layers.Dense(32,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal))\n",
        "model.add(layers.Dense(32,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "model.add(layers.Dense(10,activation='softmax'))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "              )\n",
        "print(type(model))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.keras.engine.sequential.Sequential'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpErwpvYcmp7"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=0, validation_data=(x_val, y_val))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CX-FKqBc56d",
        "outputId": "99a06a34-0350-4d22-e910-1f46f7f15e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history.history"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.7840714454650879,\n",
              "  0.928428590297699,\n",
              "  0.9436428546905518,\n",
              "  0.9529285430908203,\n",
              "  0.9589523673057556,\n",
              "  0.9630476236343384,\n",
              "  0.9660000205039978,\n",
              "  0.9685476422309875,\n",
              "  0.9713095426559448,\n",
              "  0.9730238318443298,\n",
              "  0.9750000238418579,\n",
              "  0.9778333306312561,\n",
              "  0.9779285788536072,\n",
              "  0.9802857041358948,\n",
              "  0.9814047813415527,\n",
              "  0.9818333387374878,\n",
              "  0.9848809242248535,\n",
              "  0.9844762086868286,\n",
              "  0.986047625541687,\n",
              "  0.9866904616355896,\n",
              "  0.9871428608894348,\n",
              "  0.9878809452056885,\n",
              "  0.989476203918457,\n",
              "  0.9878571629524231,\n",
              "  0.9902619123458862,\n",
              "  0.9907618761062622,\n",
              "  0.9908333420753479,\n",
              "  0.991357147693634,\n",
              "  0.9923333525657654,\n",
              "  0.9927856922149658,\n",
              "  0.9923571348190308,\n",
              "  0.9940237998962402,\n",
              "  0.9934999942779541,\n",
              "  0.994357168674469,\n",
              "  0.9946190714836121,\n",
              "  0.9952380657196045,\n",
              "  0.9950714111328125,\n",
              "  0.9962618947029114,\n",
              "  0.9961428642272949,\n",
              "  0.99528568983078,\n",
              "  0.996999979019165,\n",
              "  0.9970714449882507,\n",
              "  0.9961190223693848,\n",
              "  0.9975000023841858,\n",
              "  0.9964285492897034,\n",
              "  0.996833324432373,\n",
              "  0.9980000257492065,\n",
              "  0.9983333349227905,\n",
              "  0.9982619285583496,\n",
              "  0.9977142810821533,\n",
              "  0.9971428513526917,\n",
              "  0.99657142162323,\n",
              "  0.9985714554786682,\n",
              "  0.9990000128746033,\n",
              "  0.9959285855293274,\n",
              "  0.9989047646522522,\n",
              "  0.9992142915725708,\n",
              "  0.9991666674613953,\n",
              "  0.9983333349227905,\n",
              "  0.9989285469055176,\n",
              "  0.997952401638031,\n",
              "  0.9966904520988464,\n",
              "  0.9975000023841858,\n",
              "  0.999666690826416,\n",
              "  0.999833345413208,\n",
              "  0.9997380971908569,\n",
              "  0.9995952248573303,\n",
              "  0.9991190433502197,\n",
              "  0.9992380738258362,\n",
              "  0.998285710811615,\n",
              "  0.998619019985199,\n",
              "  0.9964523911476135,\n",
              "  0.9990952610969543,\n",
              "  0.9997380971908569,\n",
              "  0.9998809695243835,\n",
              "  0.999666690826416,\n",
              "  0.9989523887634277,\n",
              "  0.994857132434845,\n",
              "  0.9994761943817139,\n",
              "  0.9999762177467346,\n",
              "  0.9999762177467346,\n",
              "  0.9999523758888245,\n",
              "  0.9999762177467346,\n",
              "  0.9998809695243835,\n",
              "  0.9999285936355591,\n",
              "  0.9987857341766357,\n",
              "  0.9959762096405029,\n",
              "  0.9990952610969543,\n",
              "  0.9976666569709778,\n",
              "  0.9999523758888245,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.9954047799110413,\n",
              "  0.9974762201309204,\n",
              "  0.9999762177467346,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.9999523758888245],\n",
              " 'loss': [0.9569376111030579,\n",
              "  0.40349310636520386,\n",
              "  0.3149898052215576,\n",
              "  0.2647677958011627,\n",
              "  0.22830264270305634,\n",
              "  0.20217213034629822,\n",
              "  0.1822584569454193,\n",
              "  0.16330324113368988,\n",
              "  0.14894817769527435,\n",
              "  0.1389153003692627,\n",
              "  0.12720195949077606,\n",
              "  0.11668162047863007,\n",
              "  0.11198965460062027,\n",
              "  0.10319597274065018,\n",
              "  0.09623795002698898,\n",
              "  0.09173135459423065,\n",
              "  0.08397211879491806,\n",
              "  0.08137869089841843,\n",
              "  0.07694749534130096,\n",
              "  0.07252328842878342,\n",
              "  0.07004687190055847,\n",
              "  0.06710096448659897,\n",
              "  0.06143905967473984,\n",
              "  0.0641874447464943,\n",
              "  0.05705009400844574,\n",
              "  0.05437283590435982,\n",
              "  0.053023792803287506,\n",
              "  0.05012587085366249,\n",
              "  0.04856187850236893,\n",
              "  0.04521944746375084,\n",
              "  0.04599911347031593,\n",
              "  0.041234344244003296,\n",
              "  0.04203034192323685,\n",
              "  0.038957029581069946,\n",
              "  0.037701547145843506,\n",
              "  0.03572586923837662,\n",
              "  0.035429518669843674,\n",
              "  0.031800203025341034,\n",
              "  0.03132619708776474,\n",
              "  0.033274274319410324,\n",
              "  0.028374549001455307,\n",
              "  0.02761886827647686,\n",
              "  0.029440531507134438,\n",
              "  0.025628071278333664,\n",
              "  0.027979867532849312,\n",
              "  0.02610548958182335,\n",
              "  0.02283220738172531,\n",
              "  0.021423226222395897,\n",
              "  0.021563127636909485,\n",
              "  0.022127395495772362,\n",
              "  0.02311757579445839,\n",
              "  0.024118799716234207,\n",
              "  0.018816128373146057,\n",
              "  0.017743736505508423,\n",
              "  0.02496951073408127,\n",
              "  0.017155105248093605,\n",
              "  0.015839917585253716,\n",
              "  0.015209569595754147,\n",
              "  0.017703402787446976,\n",
              "  0.015195238403975964,\n",
              "  0.017984947189688683,\n",
              "  0.020650772377848625,\n",
              "  0.01898116245865822,\n",
              "  0.012684929184615612,\n",
              "  0.011634391732513905,\n",
              "  0.01147235557436943,\n",
              "  0.011631310917437077,\n",
              "  0.013234700076282024,\n",
              "  0.012635797262191772,\n",
              "  0.014619643799960613,\n",
              "  0.014154206961393356,\n",
              "  0.020513715222477913,\n",
              "  0.01322608720511198,\n",
              "  0.010549882426857948,\n",
              "  0.009053167887032032,\n",
              "  0.009562713094055653,\n",
              "  0.01229934673756361,\n",
              "  0.02418702095746994,\n",
              "  0.01102095004171133,\n",
              "  0.008673113770782948,\n",
              "  0.007814869284629822,\n",
              "  0.007785099558532238,\n",
              "  0.007477876264601946,\n",
              "  0.007577522657811642,\n",
              "  0.007589719723910093,\n",
              "  0.010796748101711273,\n",
              "  0.019348660483956337,\n",
              "  0.011072223074734211,\n",
              "  0.01455393061041832,\n",
              "  0.007995733059942722,\n",
              "  0.006693514995276928,\n",
              "  0.006262354552745819,\n",
              "  0.006056821905076504,\n",
              "  0.005845587234944105,\n",
              "  0.01951735094189644,\n",
              "  0.015198593959212303,\n",
              "  0.007429987657815218,\n",
              "  0.006217846181243658,\n",
              "  0.00579563993960619,\n",
              "  0.005718261934816837],\n",
              " 'val_accuracy': [0.9109444618225098,\n",
              "  0.9370555281639099,\n",
              "  0.9456111192703247,\n",
              "  0.9526110887527466,\n",
              "  0.9533888697624207,\n",
              "  0.9557777643203735,\n",
              "  0.9562777876853943,\n",
              "  0.9608333110809326,\n",
              "  0.960777759552002,\n",
              "  0.9623888731002808,\n",
              "  0.9614444375038147,\n",
              "  0.9586666822433472,\n",
              "  0.9643333554267883,\n",
              "  0.9658889174461365,\n",
              "  0.9660555720329285,\n",
              "  0.965499997138977,\n",
              "  0.9666110873222351,\n",
              "  0.9646111130714417,\n",
              "  0.964555561542511,\n",
              "  0.9670000076293945,\n",
              "  0.9682777523994446,\n",
              "  0.965666651725769,\n",
              "  0.9652222394943237,\n",
              "  0.9627777934074402,\n",
              "  0.9655555486679077,\n",
              "  0.9660555720329285,\n",
              "  0.9641110897064209,\n",
              "  0.9664999842643738,\n",
              "  0.967555582523346,\n",
              "  0.9671666622161865,\n",
              "  0.9674444198608398,\n",
              "  0.9663333296775818,\n",
              "  0.9669444561004639,\n",
              "  0.9678333401679993,\n",
              "  0.9666110873222351,\n",
              "  0.9655555486679077,\n",
              "  0.9676666855812073,\n",
              "  0.9680555462837219,\n",
              "  0.9679999947547913,\n",
              "  0.9674444198608398,\n",
              "  0.965666651725769,\n",
              "  0.9681110978126526,\n",
              "  0.9673333168029785,\n",
              "  0.9665555357933044,\n",
              "  0.9667778015136719,\n",
              "  0.9673333168029785,\n",
              "  0.9678333401679993,\n",
              "  0.9674444198608398,\n",
              "  0.968666672706604,\n",
              "  0.9671666622161865,\n",
              "  0.9649444222450256,\n",
              "  0.968500018119812,\n",
              "  0.9679444432258606,\n",
              "  0.9658889174461365,\n",
              "  0.968500018119812,\n",
              "  0.9665555357933044,\n",
              "  0.9697222113609314,\n",
              "  0.9684444665908813,\n",
              "  0.9684444665908813,\n",
              "  0.9639999866485596,\n",
              "  0.9674999713897705,\n",
              "  0.9672777652740479,\n",
              "  0.9696111083030701,\n",
              "  0.9677777886390686,\n",
              "  0.9684444665908813,\n",
              "  0.9683333039283752,\n",
              "  0.9674999713897705,\n",
              "  0.9681110978126526,\n",
              "  0.967555582523346,\n",
              "  0.9670000076293945,\n",
              "  0.9677222371101379,\n",
              "  0.9663888812065125,\n",
              "  0.9692777991294861,\n",
              "  0.9691666960716248,\n",
              "  0.9678333401679993,\n",
              "  0.9660555720329285,\n",
              "  0.9637777805328369,\n",
              "  0.9668889045715332,\n",
              "  0.9682222008705139,\n",
              "  0.9689444303512573,\n",
              "  0.9685555696487427,\n",
              "  0.9682222008705139,\n",
              "  0.9691666960716248,\n",
              "  0.968999981880188,\n",
              "  0.9696111083030701,\n",
              "  0.9687777757644653,\n",
              "  0.9652777910232544,\n",
              "  0.9631111025810242,\n",
              "  0.9678333401679993,\n",
              "  0.9690555334091187,\n",
              "  0.9684444665908813,\n",
              "  0.9696111083030701,\n",
              "  0.9700555801391602,\n",
              "  0.9697222113609314,\n",
              "  0.961722195148468,\n",
              "  0.9683333039283752,\n",
              "  0.9690555334091187,\n",
              "  0.9697222113609314,\n",
              "  0.9703333377838135,\n",
              "  0.9695000052452087],\n",
              " 'val_loss': [0.4891135096549988,\n",
              "  0.3491065204143524,\n",
              "  0.29184237122535706,\n",
              "  0.25751206278800964,\n",
              "  0.23622149229049683,\n",
              "  0.22385074198246002,\n",
              "  0.20588450133800507,\n",
              "  0.18739977478981018,\n",
              "  0.18261893093585968,\n",
              "  0.17034225165843964,\n",
              "  0.16984431445598602,\n",
              "  0.17104116082191467,\n",
              "  0.15509389340877533,\n",
              "  0.1519724428653717,\n",
              "  0.14658603072166443,\n",
              "  0.1470165252685547,\n",
              "  0.13951894640922546,\n",
              "  0.1482182741165161,\n",
              "  0.14881448447704315,\n",
              "  0.13674558699131012,\n",
              "  0.1361790895462036,\n",
              "  0.14317017793655396,\n",
              "  0.1408577710390091,\n",
              "  0.1478596329689026,\n",
              "  0.1418594866991043,\n",
              "  0.14003156125545502,\n",
              "  0.15129590034484863,\n",
              "  0.13660357892513275,\n",
              "  0.1345461755990982,\n",
              "  0.13469812273979187,\n",
              "  0.13809937238693237,\n",
              "  0.1398109793663025,\n",
              "  0.13803941011428833,\n",
              "  0.13859690725803375,\n",
              "  0.14639979600906372,\n",
              "  0.1455232948064804,\n",
              "  0.13965347409248352,\n",
              "  0.13686032593250275,\n",
              "  0.13823767006397247,\n",
              "  0.14249709248542786,\n",
              "  0.14839228987693787,\n",
              "  0.1418064534664154,\n",
              "  0.14355473220348358,\n",
              "  0.1497502475976944,\n",
              "  0.14675067365169525,\n",
              "  0.14995354413986206,\n",
              "  0.14395146071910858,\n",
              "  0.1419859081506729,\n",
              "  0.14573000371456146,\n",
              "  0.15331226587295532,\n",
              "  0.16784092783927917,\n",
              "  0.14775575697422028,\n",
              "  0.15012450516223907,\n",
              "  0.1621779501438141,\n",
              "  0.1576290726661682,\n",
              "  0.16183076798915863,\n",
              "  0.14729753136634827,\n",
              "  0.146920844912529,\n",
              "  0.15386399626731873,\n",
              "  0.17232045531272888,\n",
              "  0.1621742844581604,\n",
              "  0.16384756565093994,\n",
              "  0.15637259185314178,\n",
              "  0.1584651619195938,\n",
              "  0.1510905772447586,\n",
              "  0.1523759812116623,\n",
              "  0.15355701744556427,\n",
              "  0.15516173839569092,\n",
              "  0.16250504553318024,\n",
              "  0.16586129367351532,\n",
              "  0.16444501280784607,\n",
              "  0.18133248388767242,\n",
              "  0.16446664929389954,\n",
              "  0.1578805297613144,\n",
              "  0.16016538441181183,\n",
              "  0.17279712855815887,\n",
              "  0.18346279859542847,\n",
              "  0.1771642118692398,\n",
              "  0.1703367978334427,\n",
              "  0.1587623506784439,\n",
              "  0.15821006894111633,\n",
              "  0.15658822655677795,\n",
              "  0.15211553871631622,\n",
              "  0.15391843020915985,\n",
              "  0.1580212414264679,\n",
              "  0.17065981030464172,\n",
              "  0.1894465535879135,\n",
              "  0.20584359765052795,\n",
              "  0.17605161666870117,\n",
              "  0.1685813069343567,\n",
              "  0.16429026424884796,\n",
              "  0.15519361197948456,\n",
              "  0.15462557971477509,\n",
              "  0.1527736485004425,\n",
              "  0.21536441147327423,\n",
              "  0.1802166849374771,\n",
              "  0.16677510738372803,\n",
              "  0.16200807690620422,\n",
              "  0.15472082793712616,\n",
              "  0.1564447283744812]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kicf24WAgEAe",
        "outputId": "3b637c87-8a59-4f69-ac18-e7b5426e8012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(x_test[:3])\n",
        "print(\"predictions shape:\", predictions.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "79/79 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.9721\n",
            "test loss, test acc: [0.1267230361700058, 0.972100019454956]\n",
            "Generate predictions for 3 samples\n",
            "predictions shape: (3, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}